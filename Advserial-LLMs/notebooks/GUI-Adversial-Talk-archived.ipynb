{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Claude vs GPT\n",
    "\n",
    "**Improvements**\n",
    "- Pause the convo and continue without restarting.\n",
    "- Add paramter to disable streaming in the backend as it is unecessary for our gradio function.\n",
    "- Choose the topic ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.gui import adversarial_chat\n",
    "from src.test_chatbot import TestEris, TestSocrates\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "import soundfile as sf\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c776c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -qq -y install espeak-ng > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Debate: Claude VS GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb10ba",
   "metadata": {},
   "source": [
    "### Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93404c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "pipeline = KPipeline(lang_code='a', repo_id='hexgrad/Kokoro-82M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "# import tempfile\n",
    "# import numpy as np\n",
    "# from playsound import playsound\n",
    "\n",
    "# def speak(text, voice=\"af_heart\", save=False):\n",
    "#     generator = pipeline(text, voice=voice)\n",
    "\n",
    "#     for i, (_, _, audio) in enumerate(generator):\n",
    "#         # Convierte el tensor a numpy\n",
    "#         if hasattr(audio, \"cpu\"):\n",
    "#             audio = audio.cpu().numpy()\n",
    "        \n",
    "#         if save:\n",
    "#             sf.write(f\"{i}.wav\", audio, 24000)\n",
    "#         else:\n",
    "#             # Archivo temporal para reproducir\n",
    "#             with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpfile:\n",
    "#                 sf.write(tmpfile.name, audio, 24000)\n",
    "#                 playsound(tmpfile.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message):\n",
    "    \"\"\"\n",
    "    Replace double newlines with single newline.\n",
    "    Useful because Claude like new paragraphs but we lack space.\n",
    "    \"\"\"\n",
    "    return message.replace(\"\\n\\n\", \"\\n\").strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def speak(text, voice=\"af_heart\"):\n",
    "    generator = pipeline(text, voice=voice)\n",
    "    for i, (_, _, audio_tensor) in enumerate(generator):\n",
    "        audio_np = audio_tensor.numpy()  # convert to numpy array\n",
    "        return (24000, audio_np)  # Gradio excepts a tuple (sample_rate, np.array)\n",
    "    \n",
    "    \n",
    "def stream_message(history, message, is_socrates=True, delay=0.01):\n",
    "    partial = \"\"\n",
    "    for char in message:\n",
    "        partial += char\n",
    "        if is_socrates:\n",
    "            if history and history[-1][\"role\"] == \"user\":\n",
    "                history[-1][\"content\"] = partial\n",
    "            else:\n",
    "                history.append({\"role\": \"user\", \"content\": partial})\n",
    "        else:\n",
    "            if history and history[-1][\"role\"] == \"assistant\":\n",
    "                history[-1][\"content\"] = partial\n",
    "            else:\n",
    "                history.append({\"role\": \"assistant\", \"content\": partial})\n",
    "        time.sleep(delay)\n",
    "        yield history, None  # emitimos sin audio por ahora\n",
    "\n",
    "    # Cuando se completa el mensaje, generamos el audio\n",
    "    audio = speak(partial)  # esto ya es (24000, np.array)\n",
    "    return history, audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, message, is_socrates):\n",
    "    if is_socrates:\n",
    "        if history and history[-1][\"role\"] == \"user\":\n",
    "            history[-1][\"content\"] = message\n",
    "        else:\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "    else:\n",
    "        if history and history[-1][\"role\"] == \"assistant\":\n",
    "            history[-1][\"content\"] = message\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": message})\n",
    "    return history\n",
    "\n",
    "\n",
    "def adversarial_chat(n_rounds, topic):\n",
    "    socrates = TestSocrates()\n",
    "    eris = TestEris()\n",
    "    history = []\n",
    "\n",
    "    time.sleep(random.uniform(0.25, 0.5))\n",
    "    soc_reply = clean_message(socrates.ask(topic))\n",
    "    \n",
    "    partial = \"\"\n",
    "    for char in soc_reply:\n",
    "        partial += char\n",
    "        history = update_history(history, partial, is_socrates=True)\n",
    "        yield history, None\n",
    "\n",
    "    audio = speak(soc_reply)\n",
    "    yield history, audio  # return audio once message is fully spoken\n",
    "    time.sleep(3)\n",
    "\n",
    "    for i in range(n_rounds-1):\n",
    "        time.sleep(random.uniform(0.25, 0.5))\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            eris.history = socrates.history\n",
    "            eris_reply = clean_message(eris.ask(soc_reply))\n",
    "            partial = \"\"\n",
    "            for char in eris_reply:\n",
    "                partial += char\n",
    "                history = update_history(history, partial, is_socrates=False)\n",
    "                yield history, None\n",
    "            audio = speak(eris_reply)\n",
    "            yield history, audio\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            socrates.history = eris.history\n",
    "            soc_reply = clean_message(socrates.ask(eris_reply))\n",
    "            partial = \"\"\n",
    "            for char in soc_reply:\n",
    "                partial += char\n",
    "                history = update_history(history, partial, is_socrates=True)\n",
    "                yield history, None\n",
    "            audio = speak(soc_reply)\n",
    "            yield history, audio\n",
    "            time.sleep(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafffd1",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "781e7b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "# Gradio UI (edit only input and .click line)\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ⚔️ Socrates vs. Eris\")\n",
    "    gr.Markdown(\"...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        rounds = gr.Number(label=\"Rounds\", value=4, precision=0)\n",
    "        topic_input = gr.Textbox(label=\"Debate Topic\", placeholder=\"e.g. What is freedom?\")\n",
    "        start_btn = gr.Button(\"Start Debate\")\n",
    "\n",
    "    chatbox = gr.Chatbot(label=\"Let the battle begin!\", type=\"messages\", height=200)\n",
    "    # audio_output = gr.Audio(label=\"Current Speaker\", autoplay=True)\n",
    "\n",
    "    audio_output = gr.Audio(label=\"Current Speaker\", autoplay=True, elem_id=\"audio_player\")\n",
    "    block_audio = gr.State(value=False)\n",
    "\n",
    "    def reset_chat():\n",
    "        return [], None\n",
    "\n",
    "    start_btn.click(fn=reset_chat, outputs=[chatbox, audio_output])\n",
    "    # start_btn.click(fn=adversarial_chat,\n",
    "    #                 inputs=[rounds, topic_input],\n",
    "    #                 outputs=[chatbox, audio_output])\n",
    "    \n",
    "    start_btn.click(\n",
    "    fn=adversarial_chat,\n",
    "    inputs=[rounds, topic_input],\n",
    "    outputs=[chatbox, audio_output],\n",
    "    js=\"\"\"\n",
    "        async (chat, audio) => {\n",
    "            const player = document.getElementById(\"audio_player\").querySelector(\"audio\");\n",
    "            if (player) {\n",
    "                await new Promise((resolve) => {\n",
    "                    player.onended = resolve;\n",
    "                });\n",
    "            }\n",
    "            return [chat, audio];\n",
    "        }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1055de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
