{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Claude vs GPT\n",
    "\n",
    "**Improvements**\n",
    "- Pause the convo and continue without restarting.\n",
    "- Choose the topic ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from chat.chatbot import Socrates, Eris\n",
    "from chat.test_chatbot import TestEris, TestSocrates\n",
    "from streaming.stream import stream_with_typing_and_audio\n",
    "# from audio.test_speak import speak_message\n",
    "\n",
    "import gradio as gr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c776c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
    "# !winget install ffmpeg        # Required with pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Debate: Claude VS GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d836f85",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_chat(n_rounds, topic, speed=0.03, voice=\"kokoro\"):\n",
    "    socrates, eris = TestSocrates(), TestEris()\n",
    "    history = []\n",
    "\n",
    "    # --- opening taunt from Eris ---\n",
    "    eris_reply = random.choice([\n",
    "        f\"Ready to get schooled about '{topic}', old man?\",\n",
    "        f\"Today's topic is '{topic}'. This should be easy—your logic is about as sharp as a wet napkin.\",\n",
    "        f\"Hope you're prepared to lose spectacularly, Socrates. Topic: '{topic}'.\",\n",
    "        f\"Hey Socrates, wanna argue about '{topic}'? Enjoy losing.\",\n",
    "    ])\n",
    "    generator = stream_with_typing_and_audio(\n",
    "        history, \n",
    "        eris_reply,\n",
    "        is_socrates=False,\n",
    "        delay=speed,\n",
    "        voice=voice\n",
    "    )\n",
    "    for history, audio in generator:\n",
    "        yield history, audio\n",
    "\n",
    "    # --- remaining rounds ---\n",
    "    for turn in range(n_rounds - 1):\n",
    "        # Socrates turn -> Even turns\n",
    "        if turn % 2 == 0:                   \n",
    "            socrates.history = eris.history.copy()\n",
    "            soc_reply = socrates.stream(eris_reply)\n",
    "            generator = stream_with_typing_and_audio(\n",
    "                history, soc_reply,\n",
    "                is_socrates=True,\n",
    "                delay=speed,\n",
    "                voice=voice\n",
    "            )\n",
    "            for history, audio in generator:\n",
    "                yield history, audio\n",
    "        # Eris' turn -> Odd turns\n",
    "        else:                               \n",
    "            eris.history = socrates.history.copy()\n",
    "            eris_reply = eris.stream(soc_reply)\n",
    "            generator = stream_with_typing_and_audio(\n",
    "                history, eris_reply,\n",
    "                is_socrates=False,\n",
    "                delay=speed,\n",
    "                voice=voice\n",
    "            )\n",
    "            for history, audio in generator:\n",
    "                yield history, audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafffd1",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7229b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_with_typing_and_audio -> history: Today's topic is 'j'. This should be easy—your logic is about as sharp as a wet napkin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_with_typing_and_audio -> history: <generator object TestSocrates.stream at 0x0000027C58E88400>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2191, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1714, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 739, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 733, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 716, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 877, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\becaye.balde\\AppData\\Local\\Temp\\ipykernel_3684\\2420801198.py\", line 34, in adversarial_chat\n",
      "    for history, audio in generator:\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\notebooks\\../src\\streaming\\stream.py\", line 130, in stream_with_typing_and_audio\n",
      "    #     audio, duration = speak_message(message, is_socrates=is_socrates, voice=voice)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\notebooks\\../src\\audio\\test_speak.py\", line 36, in speak_message\n",
      "    audio = speak_with_openai_tts(message, is_socrates)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\notebooks\\../src\\audio\\openai_tts.py\", line 45, in speak_with_openai_tts\n",
      "    response = openai.audio.speech.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\openai\\resources\\audio\\speech.py\", line 99, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1239, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 958, in request\n",
      "    request = self._build_request(options, retries_taken=retries_taken)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 535, in _build_request\n",
      "    return self._client.build_request(  # pyright: ignore[reportUnknownMemberType]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 378, in build_request\n",
      "    return Request(\n",
      "           ^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\httpx\\_models.py\", line 408, in __init__\n",
      "    headers, stream = encode_request(\n",
      "                      ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\httpx\\_content.py\", line 216, in encode_request\n",
      "    return encode_json(json)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\becaye.balde\\Desktop\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\httpx\\_content.py\", line 177, in encode_json\n",
      "    body = json_dumps(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\becaye.balde\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\becaye.balde\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py\", line 200, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\becaye.balde\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py\", line 258, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\becaye.balde\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type generator is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ⚔️ Socrates vs. Eris\")\n",
    "    gr.Markdown(\n",
    "        \"Dive into a battle of minds where **Socrates**, the calm philosopher, asks piercing questions \"\n",
    "        \"to uncover truth — and **Eris**, the goddess of discord, responds with sarcastic wit and playful contradiction.  \\n\"\n",
    "        \"Choose a topic and watch them clash in a brief, animated debate.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        rounds = gr.Number(label=\"Rounds\", value=4, precision=0, minimum=1)\n",
    "        topic_input = gr.Textbox(label=\"Debate Topic\", placeholder=\"e.g. What is freedom?\")\n",
    "        start_btn = gr.Button(\"Start Debate\", interactive=True) #False)  # Initially disabled\n",
    "\n",
    "    chatbox = gr.Chatbot(label=\"Let the battle begin!\", type=\"messages\", height=500)\n",
    "    audio_output = gr.Audio(label=\"🔊 Speech\", autoplay=True)\n",
    "\n",
    "    def reset():\n",
    "        return [], None  \n",
    "\n",
    "    def validate_inputs(topic, n_rounds):\n",
    "        valid_topic = bool(topic.strip())\n",
    "        valid_rounds = isinstance(n_rounds, (int, float)) and n_rounds > 0\n",
    "        return gr.update(interactive=valid_topic and valid_rounds)\n",
    "\n",
    "    start_btn.click(fn=reset, outputs=[chatbox, audio_output])\n",
    "    start_btn.click(\n",
    "        fn=adversarial_chat, \n",
    "        inputs=[rounds, topic_input],\n",
    "        outputs=[chatbox, audio_output]\n",
    "    )\n",
    "\n",
    "    # Enable/disable start button based on both inputs\n",
    "    topic_input.change(fn=validate_inputs, inputs=[topic_input, rounds], outputs=start_btn)\n",
    "    rounds.change(fn=validate_inputs, inputs=[topic_input, rounds], outputs=start_btn)\n",
    "\n",
    "\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
