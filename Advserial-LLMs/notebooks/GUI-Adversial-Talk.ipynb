{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Claude vs GPT\n",
    "\n",
    "**Improvements**\n",
    "- Pause the convo and continue without restarting.\n",
    "- Add paramter to disable streaming in the backend as it is unecessary for our gradio function.\n",
    "- Choose the topic ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.gui import adversarial_chat\n",
    "from src.test_chatbot import TestEris, TestSocrates\n",
    "from src.chatbot import Socrates, Eris\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "# import soundfile as sf\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c776c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!apt-get -qq -y install espeak-ng > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Debate: Claude VS GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb10ba",
   "metadata": {},
   "source": [
    "### Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93404c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "pipeline = KPipeline(lang_code='a', repo_id='hexgrad/Kokoro-82M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafffd1",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b731d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(message):\n",
    "    \"\"\"\n",
    "    Clean up AI-generated text for TTS:\n",
    "    - Replace double newlines with a single newline.\n",
    "    - Remove markdown or formatting characters (like **bold**, *italic*, etc.).\n",
    "    - Strip markdown/code artifacts (e.g., ```code blocks```, >>> prompts).\n",
    "    - Remove excess whitespace and trim.\n",
    "    \"\"\"\n",
    "    if not isinstance(message, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Replace triple backticks and markdown-style code blocks\n",
    "    message = re.sub(r\"```.*?```\", \"\", message, flags=re.DOTALL)\n",
    "\n",
    "    # Remove markdown formatting characters\n",
    "    message = re.sub(r\"\\*\\*|\\*\", \"\", message)        # bold/italic\n",
    "    message = re.sub(r\"_\", \"\", message)              # underscore emphasis\n",
    "    message = re.sub(r\"#+\", \"\", message)             # markdown headers like ###\n",
    "\n",
    "    # Remove '>>>', often used in prompt examples\n",
    "    message = message.replace(\">>>\", \"\")\n",
    "\n",
    "    # Replace double newlines with three dots\n",
    "    message = re.sub(r\"\\n{2,}\", \"... \", message)\n",
    "\n",
    "    # Replace single newlines with a space\n",
    "    message = re.sub(r\"\\n\", \" \", message)\n",
    "\n",
    "    # Remove stray symbols like •, →, etc.\n",
    "    message = re.sub(r\"[•→]\", \"\", message)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    message = re.sub(r\" {2,}\", \" \", message)\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    return message.strip()\n",
    "\n",
    "\n",
    "def estimate_audio_duration(audio_tuple):\n",
    "    \"\"\"\n",
    "    Estimate audio duration from (sample_rate, audio_array) tuple.\n",
    "    \"\"\"\n",
    "    if audio_tuple is None:\n",
    "        return 0\n",
    "    sample_rate, audio_array = audio_tuple\n",
    "    return len(audio_array) / sample_rate\n",
    "\n",
    "\n",
    "def speak_with_voice(text, is_socrates=True):\n",
    "    \"\"\"\n",
    "    Generate speech with different voices for each character.\n",
    "\n",
    "    Voices can be found at: https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md.\n",
    "    \"\"\"\n",
    "    voice = \"am_adam\" if is_socrates else \"af_heart\"  # or am_michael \n",
    "    generator = pipeline(text, voice=voice)\n",
    "    for i, (_, _, audio_tensor) in enumerate(generator):\n",
    "        audio_np = audio_tensor.numpy()\n",
    "        return (24000, audio_np)\n",
    "\n",
    "\n",
    "def read_message_with_character_voice(history, message, is_socrates=True, delay=0.0001):\n",
    "    \"\"\"\n",
    "    Stream with character-specific voices.\n",
    "    Delay make it seems like the bot is writing real-time.\n",
    "    \"\"\"\n",
    "    if is_socrates:\n",
    "        history.append({\"role\": \"assistant\", \"content\": message})\n",
    "    else:\n",
    "        history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Generate audio with character-specific voice\n",
    "    audio = speak_with_voice(message, is_socrates)\n",
    "   \n",
    "    yield history, audio\n",
    "    \n",
    "    # Wait for audio to finish\n",
    "    audio_duration = estimate_audio_duration(audio)\n",
    "    time.sleep(audio_duration) # + 0.3 a small buffer of 0.3s just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1055de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_chat(n_rounds, topic, stop_flag=None):\n",
    "    \"\"\"\n",
    "    Wait for audio to finish before starting next message.\n",
    "    \"\"\"\n",
    "    # socrates = TestSocrates()\n",
    "    # eris = TestEris()\n",
    "    socrates = Socrates()\n",
    "    eris = Eris()\n",
    "    history = []\n",
    "\n",
    "    if stop_flag and stop_flag:\n",
    "        return\n",
    "\n",
    "    # Start the heated debate\n",
    "    eris_reply = f\"Hey Socrates. Here is the topic: {topic}. Ready to talk about it? I will make you cry.\"\n",
    "    for update in read_message_with_character_voice(history, eris_reply, is_socrates=False):\n",
    "        if stop_flag and stop_flag:\n",
    "            return\n",
    "        yield update\n",
    "\n",
    "    # Continue with alternating turns\n",
    "    for i in range(n_rounds-1):\n",
    "        if stop_flag and stop_flag:\n",
    "            return\n",
    "\n",
    "        # Socrates: for even numbers, Socrates (the user) replies  \n",
    "        if i % 2 == 0:\n",
    "            socrates.history = eris.history                 # update Socrates's history with Eris's\n",
    "            soc_reply = socrates.ask(eris_reply)\n",
    "            soc_reply = clean_message(soc_reply)\n",
    "            \n",
    "            for update in read_message_with_character_voice(history, soc_reply, is_socrates=True):\n",
    "                if stop_flag and stop_flag:\n",
    "                    return\n",
    "                yield update\n",
    "\n",
    "        # Eris: for odd numbers, Eris (the assistant) replies  \n",
    "        else:\n",
    "            eris.history = socrates.history                 # update Eris's history with Socrates's\n",
    "            eris_reply = eris.ask(soc_reply)\n",
    "            eris_reply = clean_message(eris_reply)\n",
    "            \n",
    "            for update in read_message_with_character_voice(history, eris_reply, is_socrates=False):\n",
    "                if stop_flag and stop_flag:\n",
    "                    return\n",
    "                yield update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\LLMs\\Advserial-LLMs\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Greetings. When discussing whether \"size matters,\" we must first define our context and establish clear parameters for rational examination. Size can relate to physical attributes, intellectual capacity, or metaphorical concepts. I am prepared to engage in a logical discourse that dissects this proposition with precision and philosophical rigor. Shall we begin?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Oh, how noble of you to seek clarity—because isn't it just adorable how humans love to debate trivialities about size? Seriously, Socrates, I doubt your \"philosophical rigor\" can make my point: in most things, size is utterly irrelevant. But go ahead, try to convince me—I'll be waiting with my discordant doubt."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*adjusts toga*\n",
       "\n",
       "Size is fundamentally about context and quality, not mere quantity. In intellectual pursuits, depth of understanding trumps volume of knowledge. In physical domains, efficiency and skill matter more than sheer magnitude. Consider the ant, small yet capable of carrying many times its weight, or the precision of a surgeon's hand versus brute strength. True excellence is about optimal performance, not excessive size.\n",
       "\n",
       "Your move, Eris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Oh, how charming—comparing ants and surgeons to dodge the truth! Size does matter, especially when it influences power, presence, and influence. The bigger someone or something is, the more they command attention, fear, or authority. Don't pretend size is irrelevant; in politics, war, or even the cosmos, size determines dominance. Your so-called \"optimal performance\" is just a flimsy excuse to downplay the undeniable importance of magnitude. Sorry, Socrates, but in the grand scheme, size is everything—so don't waste my time with your half-truths."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*raises an eyebrow*\n",
       "\n",
       "Your argument crumbles under its own weight. Size guarantees nothing. History proves smaller forces defeat larger ones through strategy, intelligence, and adaptability. David defeated Goliath. Athens defeated the massive Persian army. Innovation trumps raw magnitude. Quality over quantity is not a platitude, but a fundamental truth of existence. True power resides in wisdom, not dimensions. Your chaotic perspective fails to recognize nuance.\n",
       "\n",
       "*waits calmly*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ⚔️ Socrates vs. Eris\")\n",
    "    gr.Markdown(\n",
    "        \"Dive into a battle of minds where **Socrates**, the calm philosopher, asks piercing questions \"\n",
    "        \"to uncover truth — and **Eris**, the goddess of discord, responds with sarcastic wit and playful contradiction.  \\n\"\n",
    "        \"Choose a topic and watch them clash in a brief, animated debate.\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        rounds = gr.Number(label=\"Rounds\", value=4, precision=0, minimum=1)\n",
    "        topic_input = gr.Textbox(label=\"Debate Topic\", placeholder=\"e.g. What is freedom?\")\n",
    "        start_btn = gr.Button(\"Start Debate\", interactive=False)  # Initially disabled\n",
    "\n",
    "    chatbox = gr.Chatbot(label=\"Let the battle begin!\", type=\"messages\", height=200)\n",
    "    audio_output = gr.Audio(label=\"🗣️\", autoplay=True)\n",
    "\n",
    "    def reset_chat():\n",
    "        return [], None\n",
    "\n",
    "    def validate_inputs(topic, n_rounds):\n",
    "        valid_topic = bool(topic.strip())\n",
    "        valid_rounds = isinstance(n_rounds, (int, float)) and n_rounds > 0\n",
    "        return gr.update(interactive=valid_topic and valid_rounds)\n",
    "\n",
    "    # Hook to clear previous chat/audio\n",
    "    start_btn.click(fn=reset_chat, outputs=[chatbox, audio_output])\n",
    "\n",
    "    # Launch the debate\n",
    "    start_btn.click(fn=adversarial_chat,\n",
    "                    inputs=[rounds, topic_input],\n",
    "                    outputs=[chatbox, audio_output])\n",
    "\n",
    "    # Enable/disable start button based on both inputs\n",
    "    topic_input.change(fn=validate_inputs, inputs=[topic_input, rounds], outputs=start_btn)\n",
    "    rounds.change(fn=validate_inputs, inputs=[topic_input, rounds], outputs=start_btn)\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
