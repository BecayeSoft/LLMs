{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "import urllib\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader, PyPDFLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import DocArrayInMemorySearch, FAISS, Chroma\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and set API key\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "model_name=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the webpage as a LangChain Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the page\n",
    "# webpage = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "# webpage_path = 'data/artificial-intelligence.html'\n",
    "# urllib.request.urlretrieve(webpage, webpage_path)\n",
    "\n",
    "# # Create a loder for the page\n",
    "# loader = BSHTMLLoader(webpage_path, open_encoding='utf-8')\n",
    "# docs = loader.load()\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data/Survey of Success Factors in Data Science Project.pdf'\n",
    "\n",
    "# def extract_text_from_pdf(file_path):\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         pdf = PdfReader(file)\n",
    "#         text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "#     return text\n",
    "\n",
    "# # Extract text from the PDF and split it into sentences\n",
    "# text = extract_text_from_pdf(file_path)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with get_openai_callback() as cb:\n",
    "#     # Shorthand for the index creator\n",
    "#     index = VectorstoreIndexCreator(\n",
    "#         embedding=OpenAIEmbeddings(),\n",
    "#         vectorstore_cls=Chroma,\n",
    "#     ).from_documents(final_docs)\n",
    "\n",
    "#     query = \"What is this document about?\"\n",
    "#     response = index.query(\n",
    "#         llm=ChatOpenAI(),\n",
    "#         question=query, \n",
    "#         chain_type=\"stuff\",\n",
    "#     )\n",
    "\n",
    "#     print(cb)\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is this document about?',\n",
       " 'result': ' This document is about the Transformer, a transduction model relying entirely on self-attention to compute representations of its input and output, and the advantages it has over other models.',\n",
       " 'source_documents': [Document(page_content=', 2015.\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n11', metadata={'page': 10, 'source': 'data/Attention Is All You Need.pdf'}),\n",
       "  Document(page_content='summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an', metadata={'page': 1, 'source': 'data/Attention Is All You Need.pdf'})]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- Load the PDF ------- #\n",
    "file_path = 'data/Attention Is All You Need.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()\n",
    "\n",
    "# Split the PDF into chunks\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separator = \" \",\n",
    ")\n",
    "docs = splitter.split_documents(data)\n",
    "\n",
    "\n",
    "# ------- Vector Store ------- #\n",
    "\n",
    "# Create Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2',        # \"BAAI/bge-small-en-v1.5\"\n",
    "    encode_kwargs={'normalize_embeddings': True},               # set True to compute cosine similarity\n",
    "    query_instruction=\"Generate a representation for this sentence that can be used to retrieve related articles：\"\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "db = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "# ------- Create Retriever------- #\n",
    "\"\"\"\n",
    "\"similarity\" searches by similarity while\n",
    "\"mmr\" also searches by diversity, which means\n",
    "1st first source document would be different from the 2nd.\n",
    "\"k=2\" selects the 2 most similar chunks to the query to search from\n",
    "\"\"\"\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",             \n",
    "    search_kwargs={\"k\": 2}               # \n",
    ")\n",
    "\n",
    "\n",
    "# ------- Create QA chain ------- #                                                                                                                                                                                      \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------- Run the chain ------- #\n",
    "query = \"\"\"What is this document about?\"\"\"\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"Your answer wasn't clear, provide a more detailed one\",\n",
       " 'chat_history': [('What is this document about?',\n",
       "   ' This document is about the Transformer, a transduction model relying entirely on self-attention to compute representations of its input and output, and the advantages it has over other models.')],\n",
       " 'result': ' The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. It is based on a recurrent attention mechanism and has been shown to perform well on simple-language question answering and language modeling tasks. It is an encoder-decoder structure which maps an input sequence of symbol representations to a sequence of continuous representations, and then generates an output sequence from the representations.',\n",
       " 'source_documents': [Document(page_content='summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n)to a sequence\\nof continuous representations z= (z1,...,z n). Given z, the decoder then generates an', metadata={'page': 1, 'source': 'data/Attention Is All You Need.pdf'}),\n",
       "  Document(page_content='[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large', metadata={'page': 11, 'source': 'data/Attention Is All You Need.pdf'})]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [(query, response[\"answer\"])]\n",
    "query = \"Your answer wasn't clear, provide a more detailed one\"\n",
    "qa_chain({\"query\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce\n",
    "\n",
    "The \"map_reduce\" chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " A transformer is an architecture that uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It is used for tasks such as English constituency parsing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------- Create Retriever------- #\n",
    "\"\"\"\n",
    "\"similarity\" searches by similarity while\n",
    "\"mmr\" also searches by diversity, which means\n",
    "1st first source document would be different from the 2nd.\n",
    "\"k=2\" selects the 2 most similar chunks to the query to search from\n",
    "\"\"\"\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",         \n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "\n",
    "# ------- Create QA chain ------- #                                                                                                                                                                                      \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------- Run the chain ------- #\n",
    "query = \"\"\"What is a transformer?\"\"\"\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the response\n",
    "display(HTML(response[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine\n",
    "\n",
    "While \"map_reduce\" is fast, it provides brief answers, the \"refine\" chain type passes sequentially the output of the previous documents to the next one, thus building consistent and more complete answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "A transformer is a machine learning model used for natural language processing tasks such as language translation and English constituency parsing. It uses a stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder takes in an input sequence (x1,...,xn) of symbols and converts it into a fixed-length context vector (z). The decoder then generates an output sequence (y1,...,y m) of symbols one element at a time, with dropout being very helpful in avoiding over-fitting. In some cases, sinusoidal positional encoding may be replaced with learned positional embeddings [9], and observe nearly identical results to the base model. The transformer architecture has also been found to generalize well to English constituency parsing, with results on Section 23 of WSJ showing a WSJ 23 F1 of up to 4.33."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ------- Create QA chain ------- #                                                                                                                                                                                      \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"refine\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------- Run the chain ------- #\n",
    "query = \"\"\"What is a transformer?\"\"\"\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the response\n",
    "display(HTML(response[\"result\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
