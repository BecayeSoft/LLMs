{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(\"config.env\")\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "model_name = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and sotring documents for querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents may be too large to fit in the LLM. We can use a vector database to store the embeddings of the documents. Here is how it works:\n",
    "\n",
    "1. Embbed the documents\n",
    "    - Break documents in smmaler chunks\n",
    "    - Embeddings the chunks\n",
    "    - Save them in a vector Databse\n",
    "\n",
    "    > Documents -> Chunks () -> Embeddings -> Vector Databse\n",
    "\n",
    "2. Query the documents\n",
    "    - Create a `query`\n",
    "    - Embbed the query\n",
    "    - Compare it to vectors database\n",
    "    - Pick the N most similar vector to the query\n",
    "\n",
    "3. Send the N vectors with the prompt to the LLM\n",
    "\n",
    "\n",
    "<img src=\"imgs/vector-database.png\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "We will embbed the documents and store them in a vector store. Then we will query the documents to answer specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file = 'data/myntra_products_catalog.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "\n",
    "# Create the vector store\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    # embedding=embeddings,            # We can also manually specify the embeddings\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductID</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>ProductBrand</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Price (INR)</th>\n",
       "      <th>NumImages</th>\n",
       "      <th>Description</th>\n",
       "      <th>PrimaryColor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10017413</td>\n",
       "      <td>DKNY Unisex Black &amp; Grey Printed Medium Trolle...</td>\n",
       "      <td>DKNY</td>\n",
       "      <td>Unisex</td>\n",
       "      <td>11745</td>\n",
       "      <td>7</td>\n",
       "      <td>Black and grey printed medium trolley bag, sec...</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10016283</td>\n",
       "      <td>EthnoVogue Women Beige &amp; Grey Made to Measure ...</td>\n",
       "      <td>EthnoVogue</td>\n",
       "      <td>Women</td>\n",
       "      <td>5810</td>\n",
       "      <td>7</td>\n",
       "      <td>Beige &amp; Grey made to measure kurta with churid...</td>\n",
       "      <td>Beige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10009781</td>\n",
       "      <td>SPYKAR Women Pink Alexa Super Skinny Fit High-...</td>\n",
       "      <td>SPYKAR</td>\n",
       "      <td>Women</td>\n",
       "      <td>899</td>\n",
       "      <td>7</td>\n",
       "      <td>Pink coloured wash 5-pocket high-rise cropped ...</td>\n",
       "      <td>Pink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductID                                        ProductName ProductBrand  \\\n",
       "0   10017413  DKNY Unisex Black & Grey Printed Medium Trolle...         DKNY   \n",
       "1   10016283  EthnoVogue Women Beige & Grey Made to Measure ...   EthnoVogue   \n",
       "2   10009781  SPYKAR Women Pink Alexa Super Skinny Fit High-...       SPYKAR   \n",
       "\n",
       "   Gender  Price (INR)  NumImages  \\\n",
       "0  Unisex        11745          7   \n",
       "1   Women         5810          7   \n",
       "2   Women          899          7   \n",
       "\n",
       "                                         Description PrimaryColor  \n",
       "0  Black and grey printed medium trolley bag, sec...        Black  \n",
       "1  Beige & Grey made to measure kurta with churid...        Beige  \n",
       "2  Pink coloured wash 5-pocket high-rise cropped ...         Pink  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask question about the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, there are self-designed blue clothes. All four of the products listed have a primary color of blue and a description that includes \"self-design\".'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"Are there self-designed blue clothes?\"\n",
    "\n",
    "response = index.query(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| ProductID | ProductName | ProductBrand | Gender | Price (INR) | NumImages | Description | PrimaryColor |\n",
       "|-----------|-------------|--------------|-------|-------------|-----------|-------------|--------------|\n",
       "| 10207057 | W Women Beige & Grey Printed Straight Kurta | W | Women | 849 | 5 | Beige and Grey striped straight kurta with printed detail, has a mandarin collar with button closure, three-quarter sleeves, straight hem, and side slits | Beige |\n",
       "| 10143047 | MANGO Women Beige & Grey Printed Round Neck T-shirt | MANGO | Women | 1990 | 5 | Beige and grey printed T-shirt, has a round neck, and short sleeves | Beige |\n",
       "| 10234491 | W Women Beige & Taupe Woven Design Kurta with Palazzos & Ethnic Jacket | W | Women | 3149 | 8 | Beige, taupe and golden woven design kurta with palazzos and ethnic jacketBeige and golden woven design front open longline ethnic jacket, has a shirt collar and three-quarter sleevesTaupe and golden striped A-line calf length kurta, has a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query =\"Please list all the beige and grey women's clothes \\\n",
    "in a table in markdown and summarize each one.\"\n",
    "\n",
    "response = index.query(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging Deeper\n",
    "\n",
    "The above operation was quite easy. But what's happening under the hood? To answer this question, we will accomplish the same task step by step:\n",
    "1. Create the documents\n",
    "2. Break the documents into chunks (optional)\n",
    "3. Embbed the chunks\n",
    "4. Store the embeddings in a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Creating documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Create and load documents\n",
    "loader = CSVLoader(file_path=file)\n",
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 & 3. Embedding documents and storing them into a Vector Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents are small so we won't create chunks. We will create embeddings directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Get OpenAI embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embbed the docs and store them in a vector store\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi, my name is Becaye.\")\n",
    "print(len(embed))\n",
    "print(embed[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Query the documents**\n",
    "\n",
    "We can use the vector to store to find pieces of texts similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Suggest a white shirt for men\"\n",
    "response_docs = db.similarity_search(query)\n",
    "response_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retriever Interface that takes in a query and return a documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "# Combine the documents into a single string\n",
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
    "\n",
    "# Ask a question to the LLM\n",
    "question = \"List all the red women dresses in a table in markdown \\\n",
    "    and summarize them in an hilarious way.\"\n",
    "response = llm.call_as_llm(f\"{qdocs} Question: {question}\")\n",
    "\n",
    "# Display the response\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval QA Chain\n",
    "\n",
    "We can also create a RetrievalQA chain which takes in some arguments:\n",
    "- `llm`: the LLM model\n",
    "- `chain_type`: \"stuff\" is the simplest method. It will, well, \"stuff\" all the the documents into context and makes one call to the model.\n",
    "- `retriever` the retriever will fetch the documents to pass it to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetievalQA chain.                                                                                                                                                                                      \n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Retrieval QA Chains\n",
    "\n",
    "There are several types of retrieval methods for questions answering.\n",
    "\n",
    "### Stuff\n",
    "Stuff is the simplest method. It takes in all the documents and stuff them into the model.\n",
    "\n",
    "![stuff](imgs/stuff.png)\n",
    "\n",
    "**Pros**\n",
    "- Simple.\n",
    "\n",
    "**Cons**\n",
    "- Large documents might not fit in the context window.\n",
    "\n",
    "### Map Reduce\n",
    "Each chunk of documents is passed to the LLM independently. The results are then summarized by another LLM.\n",
    "This can be useful for summuriation of long documents.\n",
    "\n",
    "![map-reduce](imgs/map-reduce.png)\n",
    "\n",
    "**Pros**\n",
    "- Can handle large documents\n",
    "\n",
    "**Cons**\n",
    "- Makes a lot more LLM calls\n",
    "- Treat all documents independently, which might not be desirable.\n",
    "\n",
    "### Refine\n",
    "Navigate through each document by building upon the answer of the previous docs.\n",
    "\n",
    "![refine](imgs/refine.png)\n",
    "\n",
    "**Pros**\n",
    "- Build consistent responses over time.\n",
    "\n",
    "**Cons**\n",
    "- Makes a lot more LLM calls\n",
    "- Slower because it depends on the output of the previous call\n",
    "\n",
    "### Map Rerank (experimental)\n",
    "Makes a single call to the LLM for each document, ask it to return a score and select the highest score\n",
    "\n",
    "**Pros**\n",
    "- Fast\n",
    "\n",
    "**Cons**\n",
    "- Makes a lot more LLM calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
